{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is Simple Linear Regression?**\n",
        "\n",
        "Ans: Simple Linear Regression is a fundamental algorithm in machine learning used to establish a linear relationship between two variables: one independent variable (also called predictor or input feature, denoted as X) and one dependent variable (also called response or target, denoted as Y). The purpose of this algorithm is to predict the value of Y based on a given value of X.\n",
        "\n",
        "The relationship is modeled using the linear equation:\n",
        "\n",
        "\n",
        "Y=mX+c\n",
        "Here:\n",
        "\n",
        "Y is the predicted output (dependent variable),\n",
        "\n",
        "X is the input feature (independent variable),\n",
        "\n",
        "m is the slope of the regression line (it shows how much Y changes with a one-unit increase in X),\n",
        "\n",
        "c is the intercept (the value of Y when X = 0)."
      ],
      "metadata": {
        "id": "GfTpVvjCX9ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. What are the key assumptions of Simple Linear Regression?**\n",
        "\n",
        "Ans: The key assumptions of Simple Linear Regression are as follows:\n",
        "\n",
        "1) Linearity\n",
        "There is a linear relationship between the independent variable (X) and the dependent variable (Y). That is, changes in X result in proportional changes in Y.\n",
        "\n",
        "2) Independence of Errors\n",
        "The residuals (errors) are independent. This means that the error term of one observation is not correlated with that of another. It implies no autocorrelation (important in time-series data).\n",
        "\n",
        "3) Homoscedasticity\n",
        "The variance of the residuals is constant across all values of the independent variable. In other words, the spread of the errors should be roughly the same at all levels of X.\n",
        "\n",
        "4) Normality of Errors\n",
        "The residuals (differences between observed and predicted values) should be normally distributed, especially for hypothesis testing and confidence intervals to be valid.\n",
        "\n",
        "5) No Multicollinearity (not applicable in Simple Linear Regression)\n",
        "This assumption applies to multiple linear regression. In simple linear regression, since there is only one independent variable, multicollinearity is not a concern."
      ],
      "metadata": {
        "id": "hMp7FyVOY0EP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What does the coefficient m represent in the equation Y=mX+c?**\n",
        "\n",
        "Ans: In the equation Y = mX + c, the coefficient m represents the slope of the regression line.\n",
        "\n",
        "What It Means:\n",
        "m quantifies the change in the dependent variable (Y) for a one-unit increase in the independent variable (X).\n",
        "\n",
        "In other words, m tells how much Y increases or decreases when X increases by 1.\n",
        "\n",
        "Interpretation:\n",
        "If m > 0: There is a positive relationship between X and Y (as X increases, Y increases).\n",
        "\n",
        "If m < 0: There is a negative relationship between X and Y (as X increases, Y decreases).\n",
        "\n",
        "If m = 0: There is no relationship; Y remains constant regardless of X."
      ],
      "metadata": {
        "id": "1WLJGdS1ZMtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What does the intercept c represent in the equation Y=mX+c?**\n",
        "\n",
        "Ans: In the equation Y = mX + c, the intercept c represents the value of Y when X = 0.\n",
        "\n",
        "Meaning:\n",
        "It is the point where the regression line crosses the Y-axis.\n",
        "\n",
        "It shows the baseline value of the dependent variable (Y) when there is no contribution from the independent variable (X).\n",
        "\n",
        "Interpretation:\n",
        "If X = 0, then:\n",
        "\n",
        "\n",
        "Y=m(0)+c=c\n",
        "So, c is the predicted value of Y at that point."
      ],
      "metadata": {
        "id": "6VqcGHCEZmg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.  How do we calculate the slope m in Simple Linear Regression?**\n",
        "\n",
        "Ans:In Simple Linear Regression, the slope (m) is calculated using the Least Squares Method, which minimizes the sum of squared differences between actual and predicted values."
      ],
      "metadata": {
        "id": "avUV7aRXZm2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. What is the purpose of the least squares method in Simple Linear Regression?**\n",
        "\n",
        "Ans: The purpose of the least squares method in Simple Linear Regression is to find the best-fitting line through the data by minimizing the sum of the squared differences between the actual values and the predicted values of the dependent variable (Y)."
      ],
      "metadata": {
        "id": "wUepTLbaZm5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. How is the coefficient of determination (RÂ²) interpreted in Simple Linear Regression?**\n",
        "\n",
        "Ans:In Simple Linear Regression, the coefficient of determination (RÂ²) measures how well the regression line explains the variability of the dependent variable (Y) based on the independent variable (X).\n"
      ],
      "metadata": {
        "id": "z8jBnS0QZm7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. What is Multiple Linear Regression.**\n",
        "\n",
        "Ans: Multiple Linear Regression is a supervised machine learning algorithm that models the relationship between one dependent variable (Y) and two or more independent variables (Xâ‚, Xâ‚‚, Xâ‚ƒ, ..., Xâ‚™) using a linear equation.\n",
        "\n",
        " Mathematical Equation:\n",
        "ð‘Œ=ð‘0+ð‘1ð‘‹1+ð‘2ð‘‹2+ð‘3ð‘‹3+â‹¯+ð‘ð‘›ð‘‹ð‘›\n",
        "Where:\n",
        "\n",
        "Y = Dependent variable (target)\n",
        "\n",
        "Xâ‚, Xâ‚‚, ..., Xâ‚™ = Independent variables (features)\n",
        "\n",
        "bâ‚€ = Intercept (value of Y when all X's are 0)\n",
        "\n",
        "bâ‚, bâ‚‚, ..., bâ‚™ = Coefficients (slopes) for each feature"
      ],
      "metadata": {
        "id": "ekGNaPesZm91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. What is the main difference between Simple and Multiple Linear Regression?**\n",
        "\n",
        "Ans:The main difference between Simple Linear Regression and Multiple Linear Regression lies in the number of independent variables used to predict the dependent variable.\n",
        "\n",
        " Simple Linear Regression:\n",
        "Uses one independent variable (X)\n",
        "\n",
        "Equation:\n",
        "\n",
        "\n",
        "Y=mX+c\n",
        "Models a straight-line relationship between one input and one output.\n",
        "\n",
        " Multiple Linear Regression:\n",
        "Uses two or more independent variables (Xâ‚, Xâ‚‚, ..., Xâ‚™)\n",
        "\n",
        "Equation:\n",
        "\n",
        "ð‘Œ=ð‘0+ð‘1ð‘‹1+ð‘2ð‘‹2+â‹¯+ð‘ð‘›ð‘‹ð‘›\n",
        "\n",
        "Models how multiple features simultaneously affect the output.\n",
        "\n"
      ],
      "metadata": {
        "id": "gXb7eaotdf5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. What are the key assumptions of Multiple Linear Regression?**\n",
        "\n",
        "Ans:The key assumptions of Multiple Linear Regression ensure that the model provides valid, reliable, and interpretable results. These are:\n",
        "\n",
        "1. Linearity\n",
        "There is a linear relationship between the dependent variable (Y) and each independent variable (Xâ‚, Xâ‚‚, ..., Xâ‚™). This means changes in predictors lead to proportional changes in the target.\n",
        "\n",
        "2. Independence of Errors\n",
        "The residuals (errors) are independent of each other. This means the error for one observation does not influence another (no autocorrelation).\n",
        "\n",
        "3. Homoscedasticity\n",
        "The variance of the residuals is constant across all levels of the independent variables. In other words, the spread of errors should be uniform across predicted values.\n",
        "\n",
        "4. Normality of Errors\n",
        "The residuals (differences between actual and predicted Y) should be normally distributed, especially important for valid confidence intervals and hypothesis testing.\n",
        "\n",
        "5. No Multicollinearity\n",
        "The independent variables should not be highly correlated with each other. High multicollinearity can make it difficult to determine the individual effect of each variable on Y."
      ],
      "metadata": {
        "id": "OjN7YMzydf7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11.  What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**\n",
        "\n",
        "Ans: Heteroscedasticity occurs when the variance of the residuals (errors) is not constant across the range of predicted values or across observations. In a residualâ€‘versusâ€‘fitted plot, it often shows up as a â€œfanâ€ or â€œconeâ€ pattern rather than a random, equally spread cloud.\n",
        "\n",
        "\n",
        "Detecting it\n",
        "Visual diagnostic: plot residuals vs. fitted values and look for patterns (fanning, funnel, or clumping).\n",
        "\n",
        "Formal tests:\n",
        "\n",
        "Breuschâ€“Pagan and Koenkerâ€™s studentized Breuschâ€“Pagan tests\n",
        "\n",
        "Whiteâ€™s test (general heteroscedasticity)\n",
        "\n",
        "Goldfeldâ€“Quandt test (variance changes with an ordered variable)\n",
        "\n",
        "Dealing with it\n",
        "Robust (heteroscedasticityâ€‘consistent) standard errors\n",
        "\n",
        "E.g., White/HC0â€“HC3 in stats packages; coefficients stay the same, but inference becomes valid.\n",
        "\n",
        "Transformation of the dependent variable\n",
        "\n",
        "Log, square root, Boxâ€‘Cox to stabilize variance.\n",
        "\n",
        "Weighted Least Squares (WLS)\n",
        "\n",
        "Give each observation a weight inversely proportional to its error variance if that variance can be modeled.\n",
        "\n",
        "Model the variance explicitly\n",
        "\n",
        "Use GLS or varianceâ€function modelling (e.g., in generalized least squares).\n",
        "\n",
        "Reâ€‘specify the model\n",
        "\n",
        "Omittedâ€‘variable bias or incorrect functional form can create apparent heteroscedasticity; adding missing variables or interaction terms can help."
      ],
      "metadata": {
        "id": "aT21Fuyxdf95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12.  How can you improve a Multiple Linear Regression model with high multicollinearity?**\n",
        "\n",
        "Ans: To improve a Multiple Linear Regression model with high multicollinearity, you need to detect and then address the correlation among independent variables, which can distort the model's coefficients and reduce interpretability.\n",
        "\n",
        "Ways to Improve the Model:\n",
        "\n",
        "1. Remove Highly Correlated Predictors\n",
        "Drop one of the variables that are highly correlated (e.g., if X1 and X2 have correlation > 0.9, remove one).\n",
        "\n",
        "Choose the one that is less relevant or has a higher p-value.\n",
        "\n",
        "2. Combine Variables\n",
        "Create a composite feature (e.g., average, sum, or index) if the correlated variables measure the same concept.\n",
        "\n",
        "Example: Combine \"height in inches\" and \"height in cm\" into one.\n",
        "\n",
        "3. Use Principal Component Analysis (PCA)\n",
        "PCA transforms correlated variables into a smaller number of uncorrelated components.\n",
        "\n",
        "Use these principal components as new predictors in regression.\n",
        "\n",
        "4. Apply Regularization Techniques\n",
        "Ridge Regression (L2 penalty): Reduces coefficient size and handles multicollinearity without removing variables.\n",
        "\n",
        "Lasso Regression (L1 penalty): Performs both regularization and feature selection, can shrink some coefficients to zero.\n",
        "\n",
        "5. Center and Standardize Variables\n",
        "Subtract the mean and divide by the standard deviation to reduce numerical issues that can amplify multicollinearity.\n",
        "\n",
        "6. Collect More Data\n",
        "More observations may help stabilize the estimates, especially if multicollinearity is due to small sample size.\n",
        "\n"
      ],
      "metadata": {
        "id": "sBQdXJVGdgAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13.  What are some common techniques for transforming categorical variables for use in regression models?**\n",
        "\n",
        "Ans:To use categorical variables in regression models, you need to transform them into a numerical format, since regression algorithms require numerical input. Here are some of the most common techniques:\n",
        "\n",
        "1. One-Hot Encoding\n",
        "Creates a binary column for each category.\n",
        "\n",
        "Value is 1 if the category is present, otherwise 0.\n",
        "\n",
        "Example (for feature Color with values Red, Blue, Green):\n",
        "\n",
        "\n",
        "Color_Red   Color_Blue   Color_Green\n",
        "   1            0             0\n",
        "   0            1             0\n",
        "   0            0             1\n",
        " Best for nominal variables (no inherent order).\n",
        "\n",
        "2. Label Encoding\n",
        "Assigns a unique integer to each category.\n",
        "\n",
        "Example (for Color):\n",
        "\n",
        "\n",
        "Red = 0, Blue = 1, Green = 2\n",
        "Problem: Introduces a false sense of order, so not ideal for linear regression unless the categories are ordinal.\n",
        "\n",
        "3. Ordinal Encoding\n",
        "Similar to label encoding but preserves the natural order of categories.\n",
        "\n",
        "Example (for Size: Small < Medium < Large):\n",
        "\n",
        "Small = 1, Medium = 2, Large = 3\n",
        " Best for ordinal variables (ordered categories).\n",
        "\n",
        "4. Binary Encoding\n",
        "Converts categories to binary numbers, then splits binary digits into separate columns.\n",
        "\n",
        "Example (A=1, B=10, C=11) â†’ Columns: bit1, bit2\n",
        "\n",
        " Useful when high cardinality exists (many unique categories).\n",
        "\n",
        "5. Target Encoding (Mean Encoding)\n",
        "Replaces each category with the mean of the target variable for that category.\n",
        "\n",
        "Example: For a binary target Y (e.g., purchase = 1, no purchase = 0), replace category \"Red\" with average Y value for Red.\n",
        "\n",
        "\n",
        "6. Frequency or Count Encoding\n",
        "Replace each category with its frequency (number of occurrences) in the dataset.\n",
        "\n",
        "Example:\n",
        "Red (50), Blue (30), Green (20)\n",
        " Quick and useful when dealing with categorical features with many levels."
      ],
      "metadata": {
        "id": "BnZcOd6AdgCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14.  What is the role of interaction terms in Multiple Linear Regression?**\n",
        "\n",
        "Ans:n Multiple Linear Regression, interaction terms are used to capture the combined effect of two or more independent variables on the dependent variable, beyond their individual effects."
      ],
      "metadata": {
        "id": "M4T4XhxadgFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**\n",
        "\n",
        "Ans: The interpretation of the intercept in Simple and Multiple Linear Regression differs mainly due to the number of independent variables involved and how they are handled in the model.\n",
        "\n",
        "- In Simple Linear Regression:\n",
        "\n",
        "Y=mX+c\n",
        "The intercept (c) represents the predicted value of Y when X = 0.\n",
        "\n",
        "It is straightforward and often easy to interpret.\n",
        "\n",
        "Example:\n",
        "If you're predicting salary based on years of experience:\n",
        "\n",
        "The intercept is the estimated salary when experience = 0 years.\n",
        "\n",
        "- In Multiple Linear Regression:\n",
        "ð‘Œ=ð‘0+ð‘1ð‘‹1+ð‘2ð‘‹2+â‹¯+ð‘ð‘›ð‘‹ð‘›\n",
        "\n",
        "The intercept (bâ‚€) represents the predicted value of Y when all independent variables (Xâ‚, Xâ‚‚, ..., Xâ‚™) are equal to 0.\n",
        "\n",
        "This can be harder to interpret, especially when:\n",
        "\n",
        "Some variables canâ€™t realistically be zero (e.g., height, income).\n",
        "\n",
        "The combination of all X's being zero doesn't make practical sense."
      ],
      "metadata": {
        "id": "NkDUptuudgHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16.  What is the significance of the slope in regression analysis, and   how  does it affect predictions?**\n",
        "\n",
        "Ans: In regression analysis, the slope represents the estimated change in the dependent variable (Y) for a one-unit increase in an independent variable (X), assuming all other variables are held constant.\n",
        "\n",
        "Significance of the Slope:\n",
        "Magnitude:\n",
        "Indicates the strength of the relationship between X and Y. A larger absolute value means a stronger effect on the dependent variable.\n",
        "\n",
        "Sign (positive or negative):\n",
        "\n",
        "A positive slope means Y increases as X increases.\n",
        "\n",
        "A negative slope means Y decreases as X increases.\n",
        "\n",
        "Statistical significance:\n",
        "\n",
        "A hypothesis test (typically a t-test) is used to determine if the slope is significantly different from zero.\n",
        "\n",
        "A significant slope suggests that the variable meaningfully contributes to predicting Y.\n",
        "\n",
        "Effect on Predictions:\n",
        "The slope determines how changes in the input variable X affect the predicted value of Y.\n",
        "\n",
        "If the slope is zero, X has no effect on Y in the model.\n",
        "\n",
        "The slope is a critical component in making predictions because it defines the rate of change of Y with respect to X.\n"
      ],
      "metadata": {
        "id": "rPfC2SYDdgJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**17. How does the intercept in a regression model provide context for the relationship between variables?**\n",
        "\n",
        "Ans:The intercept in a regression model provides context by representing the expected value of the dependent variable (Y) when all independent variables (Xâ‚, Xâ‚‚, ..., Xâ‚™) are equal to zero.\n",
        "\n",
        "How It Provides Context:\n",
        "Baseline Reference Point:\n",
        "The intercept acts as a starting point or baseline prediction before accounting for the effects of any independent variables.\n",
        "\n",
        "Anchor for the Regression Line:\n",
        "It determines where the regression line crosses the Y-axis (i.e., when all X variables are 0).\n",
        "\n",
        "Interpretation Depends on the Variables:\n",
        "\n",
        "If zero is a meaningful value for all predictors, the intercept can be interpreted directly (e.g., salary at 0 years of experience).\n",
        "\n",
        "If zero is not realistic or outside the range of the data (e.g., 0 kg weight, 0 years of education), the intercept becomes more of a mathematical artifact than a practically interpretable value.\n",
        "\n",
        "Important for Prediction:\n",
        "Even if it lacks real-world meaning, the intercept is essential for accurate predictions, as it adjusts the modelâ€™s output to fit the data correctly."
      ],
      "metadata": {
        "id": "pCtaKAPhlSS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**18. What are the limitations of using RÂ² as a sole measure of model performance?**\n",
        "\n",
        "Ans: Using RÂ² (coefficient of determination) as the sole measure of model performance has several important limitations, especially in the context of regression modeling. While RÂ² tells you how much of the variance in the dependent variable is explained by the model, relying only on it can be misleading.\n",
        "\n",
        "1. RÂ² Always Increases with More Predictors\n",
        "Adding more independent variables to a model (even if theyâ€™re irrelevant) will never decrease RÂ².\n",
        "\n",
        "This can create a false sense of improvement in model performance.\n",
        "\n",
        "Thatâ€™s why Adjusted RÂ² is often used, as it penalizes the inclusion of unnecessary variables.\n",
        "\n",
        "2. Does Not Indicate Predictive Accuracy\n",
        "A high RÂ² does not guarantee good predictions, especially on new/unseen data.\n",
        "\n",
        "It does not reflect model overfitting or generalization ability.\n",
        "\n",
        "3. Insensitive to Bias\n",
        "RÂ² only measures the proportion of variance explained â€” it does not tell whether predictions are biased or whether the model is systematically wrong in any direction."
      ],
      "metadata": {
        "id": "-8v-hehjlSgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19. How would you interpret a large standard error for a regression coefficient?**\n",
        "\n",
        "Ans: A large standard error for a regression coefficient indicates that the estimate of that coefficient is not stable or precise. This means that small changes in the data could result in large changes in the estimated value of that coefficient. It often suggests that thereâ€™s:\n",
        "\n",
        "High variability in the predictor,\n",
        "\n",
        "Multicollinearity (correlation with other predictors),\n",
        "\n",
        "Or not enough data to confidently estimate its effect.\n",
        "\n",
        "The standard error is used to construct confidence intervals and to perform t-tests. A large standard error means:\n",
        "\n",
        "Wider confidence intervals, which reflect less certainty.\n",
        "\n",
        "Higher p-values, making it less likely to reject the null hypothesis that the coefficient is zero.\n",
        "This can result in potentially important predictors being dismissed because of insufficient statistical significance due to high uncertainty.\n",
        "\n"
      ],
      "metadata": {
        "id": "9ggF6DQjlSjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?**\n",
        "\n",
        "Ans: Heteroscedasticity occurs when the variance of residuals is not constant across the range of fitted values or predictors. This violates a fundamental assumption of Ordinary Least Squares (OLS) regression.\n",
        "\n",
        "Identifying heteroscedasticity:\n",
        "\n",
        "Use a residuals vs. fitted values plot. If the residuals fan out or narrow as the fitted values increase, this indicates heteroscedasticity.\n",
        "\n",
        "Residual plots may show a cone shape, wave pattern, or clusters.\n",
        "\n",
        "Formal tests include Breuschâ€“Pagan test, Whiteâ€™s test, and Goldfeldâ€“Quandt test.\n",
        "\n",
        "Why it matters:\n",
        "\n",
        "It doesnâ€™t bias the coefficients themselves but leads to inefficient estimates.\n",
        "\n",
        "Standard errors become incorrect, leading to invalid p-values and confidence intervals.\n",
        "\n",
        "This affects inference and prediction, as it can give a false sense of precision.\n",
        "\n",
        "To correct it, we can use:\n",
        "\n",
        "Robust standard errors (heteroscedasticity-consistent),\n",
        "\n",
        "Transformations (like log of Y),\n",
        "\n",
        "Weighted Least Squares, or\n",
        "\n",
        "Re-specifying the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "-K17z6SnlSmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**21. What does it mean if a Multiple Linear Regression model has a high RÂ² but low adjusted RÂ²?**\n",
        "\n",
        "Ans: A high RÂ² but low adjusted RÂ² suggests that the model may have too many irrelevant or redundant predictors.\n",
        "\n",
        "RÂ² always increases or stays the same when you add more variables, even if they donâ€™t contribute meaningfully.\n",
        "\n",
        "Adjusted RÂ² compensates for this by penalizing the addition of predictors that donâ€™t improve the modelâ€™s explanatory power.\n",
        "\n",
        "This situation indicates overfitting - where the model fits the training data well but may perform poorly on new or unseen data. It shows that the modelâ€™s complexity is not justified by its improvement in predictive power.\n",
        "\n"
      ],
      "metadata": {
        "id": "RVDT8PWBlSpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**22. Why is it important to scale variables in Multiple Linear Regression?**\n",
        "\n",
        "Ans: Scaling variables (e.g., using standardization or normalization) is important in multiple regression for several reasons:\n",
        "\n",
        "Numerical Stability:\n",
        "Predictors with large ranges can dominate calculations, causing computational instability or rounding errors.\n",
        "\n",
        "Comparability:\n",
        "Coefficients become easier to interpret on a standardized scale, especially when comparing their relative effects.\n",
        "\n",
        "Required for Regularization:\n",
        "Methods like Ridge and Lasso Regression are sensitive to the magnitude of features. Without scaling, these methods might shrink or eliminate variables unfairly.\n",
        "\n",
        "Improved Convergence:\n",
        "Algorithms used for fitting regression (e.g., gradient descent) may converge faster when features are scaled.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ei3DboQKlSsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**23. What is polynomial regression?**\n",
        "\n",
        "Ans: Polynomial regression is a type of regression analysis that models the relationship between the independent variable and the dependent variable as an nth-degree polynomial.\n",
        "\n",
        "While linear regression fits a straight line, polynomial regression fits a curved line by adding higher powers of the predictor variable(s). It allows the model to capture non-linear trends in the data.\n",
        "\n",
        "For example, instead of modeling:\n",
        "\n",
        "ð‘Œ=ð‘0+ð‘1ð‘‹\n",
        "Polynomial regression models:\n",
        "\n",
        "ð‘Œ=ð‘0+ð‘1ð‘‹+ð‘2ð‘‹2+ð‘3ð‘‹3+â‹¯+ð‘ð‘›ð‘‹ð‘›\n",
        "\n",
        "\n",
        "Itâ€™s still considered a linear model in parameters, even though the relationship between X and Y is non-linear."
      ],
      "metadata": {
        "id": "2bLx1_F3l_c_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**24.  How does polynomial regression differ from linear regression?**\n",
        "\n",
        "Ans: The key difference is in how they model the relationship between variables:\n",
        "\n",
        "Linear regression assumes a straight-line relationship between X and Y.\n",
        "\n",
        "Polynomial regression models curves, using powers of the independent variable(s).\n",
        "\n",
        "While linear regression is simple and interpretable, it cannot capture curvature. Polynomial regression is more flexible but also more prone to overfitting, especially with high-degree polynomials. The model complexity increases with the degree, and the number of terms increases rapidly.\n",
        "\n"
      ],
      "metadata": {
        "id": "l3gwJB8xl_nG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**25. When is polynomial regression used?**\n",
        "\n",
        "Ans: Polynomial regression is used when:\n",
        "\n",
        "The data shows a non-linear trend that a straight line cannot capture.\n",
        "\n",
        "Residual plots from a linear model show systematic patterns or curvature.\n",
        "\n",
        "We need a more flexible model without switching to non-parametric or tree-based methods.\n",
        "\n",
        "**Common examples include:**\n",
        "\n",
        "Modeling growth curves,\n",
        "\n",
        "Economics and pricing curves,\n",
        "\n",
        "Physical processes with known non-linear behavior."
      ],
      "metadata": {
        "id": "oP-Xic4Kl_p1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**26. What is the general equation for polynomial regression?**\n",
        "\n",
        "Ans: The general form for a univariate polynomial regression of degree\n",
        "ð‘›\n",
        "n is:\n",
        "\n",
        "ð‘Œ=ð‘0+ð‘1ð‘‹+ð‘2ð‘‹2+ð‘3ð‘‹3+â‹¯+ð‘ð‘›ð‘‹ð‘›\n",
        "Here,\n",
        "\n",
        "X is the predictor variable, and each\n",
        "ð‘ð‘–is a coefficient to be estimated. This equation allows for fitting a curve, where each term adds more flexibility to the shape.\n"
      ],
      "metadata": {
        "id": "PMTPivoBl_s2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**27.Can polynomial regression be applied to multiple variables?**\n",
        "\n",
        "Ans: Yes.This is called multivariate polynomial regression. In this case, you include:\n",
        "\n",
        "Higher-order terms of each variable (e.g.,$X_1^2,X_2^3$)\n",
        "\n",
        "Interaction terms (e.g.,$ð‘‹_1â‹…ð‘‹_2$)\n",
        "\n",
        "$Y=b\n",
        "0\n",
        "â€‹\n",
        " +b\n",
        "1\n",
        "â€‹\n",
        " X\n",
        "1\n",
        "â€‹\n",
        " +b\n",
        "2\n",
        "â€‹\n",
        " X\n",
        "1\n",
        "2\n",
        "â€‹\n",
        " +b\n",
        "3\n",
        "â€‹\n",
        " X\n",
        "2\n",
        "â€‹\n",
        " +b\n",
        "4\n",
        "â€‹\n",
        " X\n",
        "2\n",
        "2\n",
        "â€‹\n",
        " +b\n",
        "5\n",
        "â€‹\n",
        " X\n",
        "1\n",
        "â€‹\n",
        " X\n",
        "2\n",
        "â€‹\n",
        "$\n",
        "\n",
        "The number of terms grows rapidly with the number of variables and the degree, so it can become computationally expensive and harder to interpret."
      ],
      "metadata": {
        "id": "GiOyszlRl_vl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**28. What are the limitations of polynomial regression?**\n",
        "\n",
        "Ans: Overfitting:\n",
        "Higher-degree polynomials can fit noise instead of signal, reducing generalization.\n",
        "\n",
        "Poor extrapolation:\n",
        "Predictions outside the training data range can be extremely inaccurate.\n",
        "\n",
        "Multicollinearity:\n",
        "Polynomial terms like\n",
        "$ð‘‹\n",
        ",\n",
        "ð‘‹\n",
        "^2\n",
        ",\n",
        "ð‘‹\n",
        "^3$\n",
        "  are often highly correlated, making coefficient estimates unstable.\n",
        "\n",
        "Interpretability:\n",
        "Coefficients become harder to interpret as the degree increases.\n",
        "\n",
        "Computational cost:\n",
        "With many variables and high degrees, the model becomes large and slow to compute."
      ],
      "metadata": {
        "id": "8Et-Z753l_y4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?**\n",
        "\n",
        "Ans: Cross-validation:\n",
        "Splitting data into train/test folds and measuring average error helps avoid overfitting.\n",
        "\n",
        "Adjusted RÂ²:\n",
        "Improves upon RÂ² by penalizing unnecessary complexity.\n",
        "\n",
        "AIC / BIC (Information Criteria):\n",
        "Lower values indicate better model balance between fit and simplicity.\n",
        "\n",
        "RMSE / MAE:\n",
        "Useful for assessing prediction error on validation data.\n",
        "\n",
        "We should choose the lowest degree that captures the trend without overfitting, based on validation performance."
      ],
      "metadata": {
        "id": "64s2Lx7quQg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**31. How is polynomial regression implemented in Python?**\n",
        "\n",
        "Using scikit-learn:\n"
      ],
      "metadata": {
        "id": "mRJJWuZCuxCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 6, 14, 28, 45])\n",
        "\n",
        "# Degree of the polynomial\n",
        "degree = 2\n",
        "\n",
        "# Build a pipeline: polynomial transformation + linear regression\n",
        "model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X)"
      ],
      "metadata": {
        "id": "lmh0aQoZu55o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}